# Inference Log Analytics Engine

> Production-grade toolkit for analyzing AI inference logs, detecting anomalies, and optimizing performance.

## ğŸ¯ Purpose

Real-world AI deployments generate massive logs. This project provides:
- **Log Parsing**: Extract structured metrics from various inference frameworks
- **Statistical Analysis**: Latency distributions, percentiles, outliers
- **Anomaly Detection**: Identify performance regressions and failures
- **Visualization**: Interactive dashboards for exploration
- **Alerting**: Rule-based and ML-based alerting

## ğŸ“ Project Structure

```
Inference-Log-Analytics/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ onnx_runtime.py
â”‚   â”‚   â”œâ”€â”€ tensorflow_serving.py
â”‚   â”‚   â”œâ”€â”€ triton.py
â”‚   â”‚   â””â”€â”€ custom.py
â”‚   â”œâ”€â”€ analyzers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ latency.py
â”‚   â”‚   â”œâ”€â”€ throughput.py
â”‚   â”‚   â”œâ”€â”€ errors.py
â”‚   â”‚   â””â”€â”€ anomaly.py
â”‚   â”œâ”€â”€ visualizers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dashboard.py
â”‚   â”‚   â””â”€â”€ plots.py
â”‚   â””â”€â”€ alerts/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ rules.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml
â””â”€â”€ tests/
    â””â”€â”€ test_parsers.py
```

## ğŸš€ Quick Start

```python
from inference_analytics import LogAnalyzer

# Analyze ONNX Runtime logs
analyzer = LogAnalyzer.from_file("inference.log", format="onnx_runtime")
report = analyzer.analyze()

print(f"P50 Latency: {report.latency.p50:.2f}ms")
print(f"P99 Latency: {report.latency.p99:.2f}ms")
print(f"Error Rate: {report.errors.rate:.2%}")
```

## ğŸ“Š Key Metrics Extracted

| Metric | Description |
|--------|-------------|
| Latency (P50/P90/P99/P99.9) | End-to-end inference time |
| Queue Time | Time waiting before execution |
| Execution Time | Actual compute time |
| Throughput | Inferences per second |
| Error Rate | Failed inferences percentage |
| Memory Usage | Peak and average GPU memory |
| Batch Size Distribution | How requests are batched |

## ğŸ“ˆ Example Analysis

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Latency Distribution                     â”‚
â”‚                                                    â”‚
â”‚  Count                                             â”‚
â”‚    â”‚                    â–ˆâ–ˆâ–ˆâ–ˆ                       â”‚
â”‚    â”‚                 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      â”‚
â”‚    â”‚              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      â”‚
â”‚    â”‚           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      â”‚
â”‚    â”‚       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    â”‚
â”‚    â”‚   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ªâ–ª            â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ms      â”‚
â”‚        10   20   30   40   50   60   70+          â”‚
â”‚                                                    â”‚
â”‚  Stats: P50=25ms, P90=42ms, P99=58ms, P99.9=95ms â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Supported Log Formats

- **ONNX Runtime**: Native profiling output
- **TensorFlow Serving**: Request logs
- **NVIDIA Triton**: Inference server logs
- **Custom**: Configurable JSON/CSV parsers

## License

MIT
